{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doogzultz/time_series_project/blob/main/Preprocessing_and_model_testing_w_gbtrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ECqbkhf64iYw"
      },
      "source": [
        "# Preprocessing and model testing\n",
        "This code is meant to organized the data, scale it, drop the appropriate features, and train/test it on various models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZExUpyUYujQW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout, Input, \\\n",
        "Flatten, Concatenate, Conv1D, LSTM, Bidirectional, BatchNormalization, Activation, TimeDistributed, Lambda, MaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as K\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import sklearn\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, mean_absolute_percentage_error\n",
        "import xgboost\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gs1kDeKD4iZE",
        "outputId": "f0730bdc-7433-435c-848f-2d50af445b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.8.2', '1.0.2', '3.2.2', '0.90')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.__version__, sklearn.__version__, mpl.__version__, xgboost.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wlR9tqUl87mA"
      },
      "outputs": [],
      "source": [
        "''' Some global variables that we set '''\n",
        "\n",
        "class g:\n",
        "  batch_size = 32 #batch size for training nn's\n",
        "  valid_cut = '2011-08-15'# an extra split in case I wanted it for parameter tuning\n",
        "  test_cut = '2011-10-01' # the date after which we will consider data for testing\n",
        "  valid = False #valid split\n",
        "  drop_extra_factors = True #dropping factors that don't seem to be as useful from data\n",
        "  smape_threshold = .5 #threshold for the smape and mape metric\n",
        "  smape_epsilon = 1e-3 #to prevent division by 0 in smape and mape metric\n",
        "  window_size = 8 #window size for training the LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "B9G6FmAq4iZK"
      },
      "source": [
        "## Data oraganization and preprocessing\n",
        "* We organize the data into dictionaries of time series, one for each identifer.\n",
        "* We one-hot encode the 'sector' feature, and provide an option to drop all factors except 2 and 9.\n",
        "* Finally, we scale the remaining numerical features and target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D_W78yrpu3wn"
      },
      "outputs": [],
      "source": [
        "url = 'https://drive.google.com/uc?export=download&id=1sObzDP-D7DrGLkf7MXTI_NXfrvo2biRU'\n",
        "data = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rldd5P58YcUA",
        "outputId": "ef847821-e21c-44e8-c062-e7d8a317d8d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'identifier', 'market_cap', 'sector', 'index_membership',\n",
              "       'factor_1', 'factor_2', 'factor_3', 'factor_4', 'factor_5', 'factor_6',\n",
              "       'factor_7', 'factor_8', 'factor_9', 'factor_10', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dllMPyrMQ3vK",
        "outputId": "5e4c78a7-576f-42c3-afaf-33780bc9402f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32202 entries, 0 to 32201\n",
            "Data columns (total 16 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   date              32202 non-null  object \n",
            " 1   identifier        32202 non-null  object \n",
            " 2   market_cap        32202 non-null  float64\n",
            " 3   sector            32202 non-null  int64  \n",
            " 4   index_membership  32202 non-null  bool   \n",
            " 5   factor_1          32202 non-null  float64\n",
            " 6   factor_2          32202 non-null  float64\n",
            " 7   factor_3          32202 non-null  float64\n",
            " 8   factor_4          32202 non-null  float64\n",
            " 9   factor_5          32202 non-null  float64\n",
            " 10  factor_6          32202 non-null  float64\n",
            " 11  factor_7          32202 non-null  float64\n",
            " 12  factor_8          32202 non-null  float64\n",
            " 13  factor_9          32202 non-null  float64\n",
            " 14  factor_10         32202 non-null  float64\n",
            " 15  target            32202 non-null  float64\n",
            "dtypes: bool(1), float64(12), int64(1), object(2)\n",
            "memory usage: 3.7+ MB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wPLsDLlgYFO3"
      },
      "outputs": [],
      "source": [
        "''' From the data_analysis file, we know that index_membership is always True '''\n",
        "\n",
        "data.drop(columns = 'index_membership', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8IK2yjo-Z-qq",
        "outputId": "2a858d6a-d656-4f13-fd46-e00ffefb4cfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             date    identifier    market_cap  sector  factor_1  factor_2  \\\n",
              "82     2010-01-06  AD41WBQFVG43  5.843968e+10      35  0.766639 -0.000032   \n",
              "400    2010-01-13  AD41WBQFVG43  4.758865e+10      35  0.907349 -0.022933   \n",
              "718    2010-01-20  AD41WBQFVG43  3.374757e+10      35  0.476465 -0.000114   \n",
              "1036   2010-01-27  AD41WBQFVG43  4.908395e+10      35  0.265141 -0.000034   \n",
              "1354   2010-02-03  AD41WBQFVG43  2.729189e+10      35  0.423987  0.000197   \n",
              "...           ...           ...           ...     ...       ...       ...   \n",
              "12552  2010-10-06  ZMYXRS4KVOG2  6.504496e+08      25  0.069622  0.009936   \n",
              "12862  2010-10-13  ZMYXRS4KVOG2  1.403710e+09      25  0.343444  0.002566   \n",
              "13172  2010-10-20  ZMYXRS4KVOG2  7.284687e+08      25  0.857557  0.000146   \n",
              "13483  2010-10-27  ZMYXRS4KVOG2  7.766592e+08      25  0.093894 -0.000885   \n",
              "13794  2010-11-03  ZMYXRS4KVOG2  1.763706e+09      25  0.703250 -0.011134   \n",
              "\n",
              "       factor_3  factor_4  factor_5  factor_6  factor_7  factor_8  factor_9  \\\n",
              "82     0.832677  0.500608  0.193489  0.246089  0.753702  0.792736 -0.000005   \n",
              "400    0.346457  0.581694  0.648483  0.872304  0.723412  0.333230 -0.009504   \n",
              "718    0.592328  0.379747  0.810961  0.017737  0.619208  0.058918 -0.000099   \n",
              "1036   0.521405  0.682492  0.547349  0.155915  0.068966  0.699718 -0.000037   \n",
              "1354   0.950500  0.628533  0.956287  0.426634  0.752799  0.961276  0.000163   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "12552  0.588354  0.447469  0.353549  0.767779  0.790632  0.308049  0.012255   \n",
              "12862  0.394802  0.143162  0.116762  0.744089  0.825323  0.559633  0.001058   \n",
              "13172  0.892729  0.113346  0.574870  0.990100  0.319424  0.591153  0.000054   \n",
              "13483  0.311392  0.768537  0.589851  0.289180  0.431484  0.373209 -0.000411   \n",
              "13794  0.154056  0.563441  0.321925  0.180661  0.173517  0.135267 -0.020429   \n",
              "\n",
              "       factor_10    target  \n",
              "82      0.109029 -0.000006  \n",
              "400     0.121348 -0.008721  \n",
              "718     0.890898 -0.000081  \n",
              "1036    0.355059 -0.000018  \n",
              "1354    0.196711  0.000318  \n",
              "...          ...       ...  \n",
              "12552   0.281193  0.005117  \n",
              "12862   0.755124  0.000446  \n",
              "13172   0.933679  0.000035  \n",
              "13483   0.225972 -0.000242  \n",
              "13794   0.783889 -0.021317  \n",
              "\n",
              "[32202 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c51dbc12-721b-4cd3-899d-24c11b8de12e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>identifier</th>\n",
              "      <th>market_cap</th>\n",
              "      <th>sector</th>\n",
              "      <th>factor_1</th>\n",
              "      <th>factor_2</th>\n",
              "      <th>factor_3</th>\n",
              "      <th>factor_4</th>\n",
              "      <th>factor_5</th>\n",
              "      <th>factor_6</th>\n",
              "      <th>factor_7</th>\n",
              "      <th>factor_8</th>\n",
              "      <th>factor_9</th>\n",
              "      <th>factor_10</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>2010-01-06</td>\n",
              "      <td>AD41WBQFVG43</td>\n",
              "      <td>5.843968e+10</td>\n",
              "      <td>35</td>\n",
              "      <td>0.766639</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>0.832677</td>\n",
              "      <td>0.500608</td>\n",
              "      <td>0.193489</td>\n",
              "      <td>0.246089</td>\n",
              "      <td>0.753702</td>\n",
              "      <td>0.792736</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>0.109029</td>\n",
              "      <td>-0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>2010-01-13</td>\n",
              "      <td>AD41WBQFVG43</td>\n",
              "      <td>4.758865e+10</td>\n",
              "      <td>35</td>\n",
              "      <td>0.907349</td>\n",
              "      <td>-0.022933</td>\n",
              "      <td>0.346457</td>\n",
              "      <td>0.581694</td>\n",
              "      <td>0.648483</td>\n",
              "      <td>0.872304</td>\n",
              "      <td>0.723412</td>\n",
              "      <td>0.333230</td>\n",
              "      <td>-0.009504</td>\n",
              "      <td>0.121348</td>\n",
              "      <td>-0.008721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>2010-01-20</td>\n",
              "      <td>AD41WBQFVG43</td>\n",
              "      <td>3.374757e+10</td>\n",
              "      <td>35</td>\n",
              "      <td>0.476465</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>0.592328</td>\n",
              "      <td>0.379747</td>\n",
              "      <td>0.810961</td>\n",
              "      <td>0.017737</td>\n",
              "      <td>0.619208</td>\n",
              "      <td>0.058918</td>\n",
              "      <td>-0.000099</td>\n",
              "      <td>0.890898</td>\n",
              "      <td>-0.000081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>2010-01-27</td>\n",
              "      <td>AD41WBQFVG43</td>\n",
              "      <td>4.908395e+10</td>\n",
              "      <td>35</td>\n",
              "      <td>0.265141</td>\n",
              "      <td>-0.000034</td>\n",
              "      <td>0.521405</td>\n",
              "      <td>0.682492</td>\n",
              "      <td>0.547349</td>\n",
              "      <td>0.155915</td>\n",
              "      <td>0.068966</td>\n",
              "      <td>0.699718</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>0.355059</td>\n",
              "      <td>-0.000018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1354</th>\n",
              "      <td>2010-02-03</td>\n",
              "      <td>AD41WBQFVG43</td>\n",
              "      <td>2.729189e+10</td>\n",
              "      <td>35</td>\n",
              "      <td>0.423987</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.950500</td>\n",
              "      <td>0.628533</td>\n",
              "      <td>0.956287</td>\n",
              "      <td>0.426634</td>\n",
              "      <td>0.752799</td>\n",
              "      <td>0.961276</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.196711</td>\n",
              "      <td>0.000318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12552</th>\n",
              "      <td>2010-10-06</td>\n",
              "      <td>ZMYXRS4KVOG2</td>\n",
              "      <td>6.504496e+08</td>\n",
              "      <td>25</td>\n",
              "      <td>0.069622</td>\n",
              "      <td>0.009936</td>\n",
              "      <td>0.588354</td>\n",
              "      <td>0.447469</td>\n",
              "      <td>0.353549</td>\n",
              "      <td>0.767779</td>\n",
              "      <td>0.790632</td>\n",
              "      <td>0.308049</td>\n",
              "      <td>0.012255</td>\n",
              "      <td>0.281193</td>\n",
              "      <td>0.005117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12862</th>\n",
              "      <td>2010-10-13</td>\n",
              "      <td>ZMYXRS4KVOG2</td>\n",
              "      <td>1.403710e+09</td>\n",
              "      <td>25</td>\n",
              "      <td>0.343444</td>\n",
              "      <td>0.002566</td>\n",
              "      <td>0.394802</td>\n",
              "      <td>0.143162</td>\n",
              "      <td>0.116762</td>\n",
              "      <td>0.744089</td>\n",
              "      <td>0.825323</td>\n",
              "      <td>0.559633</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.755124</td>\n",
              "      <td>0.000446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13172</th>\n",
              "      <td>2010-10-20</td>\n",
              "      <td>ZMYXRS4KVOG2</td>\n",
              "      <td>7.284687e+08</td>\n",
              "      <td>25</td>\n",
              "      <td>0.857557</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.892729</td>\n",
              "      <td>0.113346</td>\n",
              "      <td>0.574870</td>\n",
              "      <td>0.990100</td>\n",
              "      <td>0.319424</td>\n",
              "      <td>0.591153</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.933679</td>\n",
              "      <td>0.000035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13483</th>\n",
              "      <td>2010-10-27</td>\n",
              "      <td>ZMYXRS4KVOG2</td>\n",
              "      <td>7.766592e+08</td>\n",
              "      <td>25</td>\n",
              "      <td>0.093894</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>0.311392</td>\n",
              "      <td>0.768537</td>\n",
              "      <td>0.589851</td>\n",
              "      <td>0.289180</td>\n",
              "      <td>0.431484</td>\n",
              "      <td>0.373209</td>\n",
              "      <td>-0.000411</td>\n",
              "      <td>0.225972</td>\n",
              "      <td>-0.000242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13794</th>\n",
              "      <td>2010-11-03</td>\n",
              "      <td>ZMYXRS4KVOG2</td>\n",
              "      <td>1.763706e+09</td>\n",
              "      <td>25</td>\n",
              "      <td>0.703250</td>\n",
              "      <td>-0.011134</td>\n",
              "      <td>0.154056</td>\n",
              "      <td>0.563441</td>\n",
              "      <td>0.321925</td>\n",
              "      <td>0.180661</td>\n",
              "      <td>0.173517</td>\n",
              "      <td>0.135267</td>\n",
              "      <td>-0.020429</td>\n",
              "      <td>0.783889</td>\n",
              "      <td>-0.021317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32202 rows Ã— 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c51dbc12-721b-4cd3-899d-24c11b8de12e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c51dbc12-721b-4cd3-899d-24c11b8de12e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c51dbc12-721b-4cd3-899d-24c11b8de12e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "''' We plan to view the data as a table of time series '''\n",
        "\n",
        "data.sort_values(by = ['identifier', 'date'], inplace = True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbWoSZgCwZxl",
        "outputId": "aa492093-d56b-46d9-d871-fe7a6deee892"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date           0\n",
              "identifier     0\n",
              "market_cap     0\n",
              "sector         0\n",
              "factor_1       0\n",
              "factor_2      68\n",
              "factor_3       0\n",
              "factor_4       0\n",
              "factor_5       0\n",
              "factor_6       0\n",
              "factor_7       0\n",
              "factor_8       0\n",
              "factor_9      68\n",
              "factor_10      0\n",
              "target        68\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "''' Checking of 0 values '''\n",
        "\n",
        "(data== 0).sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHS1Vu12vjKs",
        "outputId": "cf9c24ae-ac12-4595-8c8a-790ea77452c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "identifier\n",
              "AD41WBQFVG43    104\n",
              "UZ4DWDZ8ALZ4    104\n",
              "KR47536Y10D4    104\n",
              "KPJ8FTV9ESJ3    104\n",
              "KMHQ727PU8E2    104\n",
              "               ... \n",
              "PNUM57CYEB27      5\n",
              "KHKNKSMXBWP3      5\n",
              "LYCPQJ19AOY0      5\n",
              "RENR64FT3I74      5\n",
              "LALJLSN9UMP2      2\n",
              "Length: 356, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "''' saving identifier counts as a variable '''\n",
        "\n",
        "ident_counts= data.value_counts('identifier')\n",
        "ident_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "CeFXy6h0N-Td",
        "outputId": "f01f8295-9936-45e5-d14d-77e36959299a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7f462fe09f90>,\n",
              "  <matplotlib.axis.XTick at 0x7f462fe09f50>],\n",
              " [Text(0, 0, '2010-01-06'), Text(0, 0, '2011-12-28')])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "''' Checking the distribution of dates.\n",
        "    It seems that some date steps could have been skipped for certain identifiers '''\n",
        "\n",
        "plt.plot(data.value_counts('date').sort_index())\n",
        "plt.xticks(range(0,104,103), ('2010-01-06', '2011-12-28'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GVoPjq5wTH3B"
      },
      "outputs": [],
      "source": [
        "''' changing the data type of the date column, in order to use as orderable index '''\n",
        "\n",
        "data = data.astype({'date':np.datetime64})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n-PN9p93gMX",
        "outputId": "ad757414-4b79-4834-bfaa-555367e56981"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sector\n",
              "10    0.049221\n",
              "15    0.093162\n",
              "20    0.208527\n",
              "25    0.132849\n",
              "30    0.072759\n",
              "35    0.049003\n",
              "40    0.232812\n",
              "45    0.051674\n",
              "50    0.038352\n",
              "55    0.071642\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "''' we want to split the data into train and test sets.\n",
        "    We will split after a certain date accross all identifiers.\n",
        "    Here, we check to make sure the distribution of sectors\n",
        "        is comparable before and after the split\n",
        "        in order to give an accurate indication of performance'''\n",
        "\n",
        "\n",
        "data.value_counts('sector', normalize = True).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph3ssyi3ySWd",
        "outputId": "918901ea-4649-4800-9f7a-075f6f1d004f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sector\n",
              "10    0.049103\n",
              "15    0.093537\n",
              "20    0.207769\n",
              "25    0.133689\n",
              "30    0.071568\n",
              "35    0.049386\n",
              "40    0.234231\n",
              "45    0.051509\n",
              "50    0.037712\n",
              "55    0.071497\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "data[data['date']<g.test_cut].value_counts('sector', normalize = True).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhR4Ndd323Qx",
        "outputId": "0e47a109-a5f2-4584-ca46-03f6853fd2f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sector\n",
              "10    0.050064\n",
              "15    0.090470\n",
              "20    0.213977\n",
              "25    0.126811\n",
              "30    0.081321\n",
              "35    0.046252\n",
              "40    0.222618\n",
              "45    0.052859\n",
              "50    0.042948\n",
              "55    0.072681\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "data[data['date']>g.test_cut].value_counts('sector', normalize = True).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hxUhpeVkTh2y"
      },
      "outputs": [],
      "source": [
        "''' One-hot encoding the sectors.\n",
        "    Tried one-hot encoding identifiers, performance difference was not perceptible '''\n",
        "\n",
        "sector_onehot = pd.get_dummies(data['sector'], prefix='sector')\n",
        "#ident_onehot = pd.get_dummies(data['identifier'], prefix='ident')\n",
        "#ident_onehot.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEeZGkLaVP7i",
        "outputId": "1a3e0625-01a0-4846-c0aa-793cce6a5cb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'identifier', 'market_cap', 'factor_1', 'factor_2', 'factor_3',\n",
              "       'factor_4', 'factor_5', 'factor_6', 'factor_7', 'factor_8', 'factor_9',\n",
              "       'factor_10', 'sector_10', 'sector_15', 'sector_20', 'sector_25',\n",
              "       'sector_30', 'sector_35', 'sector_40', 'sector_45', 'sector_50',\n",
              "       'sector_55', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "''' dropping original sector column, adding one-hots, and putting target at the last column '''\n",
        "\n",
        "target = data['target']\n",
        "data.drop(columns = ['sector', 'target'], inplace = True)\n",
        "data = pd.concat([data, sector_onehot, target], axis = 1)\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Cjabx-Qy4GXk"
      },
      "outputs": [],
      "source": [
        "def split_to_series(df,valid_cut = g.valid_cut, valid = g.valid, test_cut= g.test_cut, drop_non_complete = False):\n",
        "    '''\n",
        "    puts each identifer time series into a dictionary value and outputs two or three dictionaries corresponding to train/test/valid\n",
        "    \n",
        "    args:\n",
        "        df -- dataframe\n",
        "        valid_cut -- date to cut for validation set, not used if valid = False\n",
        "        valid -- do valididation cut\n",
        "        test_cut -- date for test cut\n",
        "        drop_non_complete -- use only series with 104 entries if True\n",
        "    returns\n",
        "        train and test dictionary with numerical keys corresponding to some time sub-series\n",
        "        with valid dictionary if valid = True\n",
        "        '''\n",
        "    \n",
        "    \n",
        "    series_dict = {}\n",
        "    train_dict = {}\n",
        "    valid_dict = {0: pd.DataFrame(columns = data.columns)} #empty dataframe in case we want a valid split as well\n",
        "    test_dict = {0: pd.DataFrame(columns = data.columns)}\n",
        "    for i, ident in enumerate(df.value_counts('identifier').index):\n",
        "        #first we separate by identifier and index by date, add each to series_dict\n",
        "        #then we cut each series in series_dict by the specified dates and add the subseries to the appropriate train/test/valid dictionaries\n",
        "\n",
        "        if df[df['identifier'] == ident].shape[0]<104 and drop_non_complete == True:\n",
        "          continue\n",
        "        series_dict[i] = df[df['identifier'] == ident].set_index('date', verify_integrity= True).sort_index()\n",
        "        if valid == True:\n",
        "          #see below for explanation\n",
        "\n",
        "          train_dict[i] = series_dict[i].loc[:valid_cut]\n",
        "          valid_dict[i] = series_dict[i].loc[valid_cut:test_cut]\n",
        "          test_dict[i] = series_dict[i].loc[test_cut:]\n",
        "        else:\n",
        "          #separate each series into train and test sets, where we cut by test_cut and\n",
        "          #add the data after test_cut to test_dict\n",
        "\n",
        "          train_dict[i] = series_dict[i].loc[:test_cut]\n",
        "          test_dict[i] = series_dict[i].loc[test_cut:] \n",
        "\n",
        "    #Finally, we calculate the train split size in terms of series and time steps    \n",
        "\n",
        "    print(f\"Number of series to be trained: {len([key for key, value in train_dict.items() if value.shape[0]>0])}\")\n",
        "    print(f\"Number of series to be validated: {len([key for key, value in valid_dict.items() if value.shape[0]>0])}\")\n",
        "    print(f\"Number of series to be tested: {len([key for key, value in test_dict.items() if value.shape[0]>0])}\")\n",
        "    train_length = sum([value.shape[0] for key, value in train_dict.items()])\n",
        "    valid_length = sum([value.shape[0] for key, value in valid_dict.items()])\n",
        "    test_length = sum([value.shape[0] for key, value in test_dict.items()])\n",
        "\n",
        "    print(f\"Number of time steps to be trained: {train_length}\")\n",
        "    print(f\"Number of time steps to be validated: {valid_length}\")\n",
        "    print(f\"Number of time steps to be tested: {test_length}\")\n",
        "    print(f\"Train_proportion: {train_length/(train_length+valid_length+test_length)}\")\n",
        "\n",
        "    return train_dict, valid_dict, test_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjuTgyaN6Glh",
        "outputId": "a6237e38-b571-4cc6-ffd7-0c86d285c83a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of series to be trained: 349\n",
            "Number of series to be validated: 0\n",
            "Number of series to be tested: 313\n",
            "Number of time steps to be trained: 28267\n",
            "Number of time steps to be validated: 0\n",
            "Number of time steps to be tested: 3935\n",
            "Train_proportion: 0.8778026209552202\n"
          ]
        }
      ],
      "source": [
        "''' make the split(s), valid_dict could be empty '''\n",
        "\n",
        "train_dict, valid_dict, test_dict = split_to_series(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiU1ZefIhBMb",
        "outputId": "86f70aa7-4fa3-4c5c-8c5c-e6209da9c4d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 91 entries, 2010-01-06 to 2011-09-28\n",
            "Data columns (total 23 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   identifier  91 non-null     object \n",
            " 1   market_cap  91 non-null     float64\n",
            " 2   factor_1    91 non-null     float64\n",
            " 3   factor_2    91 non-null     float64\n",
            " 4   factor_3    91 non-null     float64\n",
            " 5   factor_4    91 non-null     float64\n",
            " 6   factor_5    91 non-null     float64\n",
            " 7   factor_6    91 non-null     float64\n",
            " 8   factor_7    91 non-null     float64\n",
            " 9   factor_8    91 non-null     float64\n",
            " 10  factor_9    91 non-null     float64\n",
            " 11  factor_10   91 non-null     float64\n",
            " 12  sector_10   91 non-null     uint8  \n",
            " 13  sector_15   91 non-null     uint8  \n",
            " 14  sector_20   91 non-null     uint8  \n",
            " 15  sector_25   91 non-null     uint8  \n",
            " 16  sector_30   91 non-null     uint8  \n",
            " 17  sector_35   91 non-null     uint8  \n",
            " 18  sector_40   91 non-null     uint8  \n",
            " 19  sector_45   91 non-null     uint8  \n",
            " 20  sector_50   91 non-null     uint8  \n",
            " 21  sector_55   91 non-null     uint8  \n",
            " 22  target      91 non-null     float64\n",
            "dtypes: float64(12), object(1), uint8(10)\n",
            "memory usage: 10.8+ KB\n",
            "identifier\n",
            "HRM5F6C5CBA1    91\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "'''checking different dictionary entries\n",
        "    '''\n",
        "\n",
        "train_dict[56].info()\n",
        "print(train_dict[56].value_counts('identifier'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3ZVl2GAiPHU",
        "outputId": "4c4dfdd3-2c6b-4e24-e38e-637742170fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 0 entries\n",
            "Data columns (total 24 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   date        0 non-null      object\n",
            " 1   identifier  0 non-null      object\n",
            " 2   market_cap  0 non-null      object\n",
            " 3   factor_1    0 non-null      object\n",
            " 4   factor_2    0 non-null      object\n",
            " 5   factor_3    0 non-null      object\n",
            " 6   factor_4    0 non-null      object\n",
            " 7   factor_5    0 non-null      object\n",
            " 8   factor_6    0 non-null      object\n",
            " 9   factor_7    0 non-null      object\n",
            " 10  factor_8    0 non-null      object\n",
            " 11  factor_9    0 non-null      object\n",
            " 12  factor_10   0 non-null      object\n",
            " 13  sector_10   0 non-null      object\n",
            " 14  sector_15   0 non-null      object\n",
            " 15  sector_20   0 non-null      object\n",
            " 16  sector_25   0 non-null      object\n",
            " 17  sector_30   0 non-null      object\n",
            " 18  sector_35   0 non-null      object\n",
            " 19  sector_40   0 non-null      object\n",
            " 20  sector_45   0 non-null      object\n",
            " 21  sector_50   0 non-null      object\n",
            " 22  sector_55   0 non-null      object\n",
            " 23  target      0 non-null      object\n",
            "dtypes: object(24)\n",
            "memory usage: 0.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "valid_dict[0].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWM8B9vPkngn",
        "outputId": "1b619c97-e7e9-47d0-fa1a-0df28a04b2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 13 entries, 2011-10-05 to 2011-12-28\n",
            "Data columns (total 23 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   identifier  13 non-null     object \n",
            " 1   market_cap  13 non-null     float64\n",
            " 2   factor_1    13 non-null     float64\n",
            " 3   factor_2    13 non-null     float64\n",
            " 4   factor_3    13 non-null     float64\n",
            " 5   factor_4    13 non-null     float64\n",
            " 6   factor_5    13 non-null     float64\n",
            " 7   factor_6    13 non-null     float64\n",
            " 8   factor_7    13 non-null     float64\n",
            " 9   factor_8    13 non-null     float64\n",
            " 10  factor_9    13 non-null     float64\n",
            " 11  factor_10   13 non-null     float64\n",
            " 12  sector_10   13 non-null     uint8  \n",
            " 13  sector_15   13 non-null     uint8  \n",
            " 14  sector_20   13 non-null     uint8  \n",
            " 15  sector_25   13 non-null     uint8  \n",
            " 16  sector_30   13 non-null     uint8  \n",
            " 17  sector_35   13 non-null     uint8  \n",
            " 18  sector_40   13 non-null     uint8  \n",
            " 19  sector_45   13 non-null     uint8  \n",
            " 20  sector_50   13 non-null     uint8  \n",
            " 21  sector_55   13 non-null     uint8  \n",
            " 22  target      13 non-null     float64\n",
            "dtypes: float64(12), object(1), uint8(10)\n",
            "memory usage: 1.5+ KB\n",
            "identifier\n",
            "HRM5F6C5CBA1    13\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "test_dict[56].info()\n",
        "print(test_dict[56].value_counts('identifier'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67TXCBBLN-lX",
        "outputId": "0c3bc47c-8095-4fa0-eae4-d7c28d091afa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['market_cap',\n",
              " 'factor_1',\n",
              " 'factor_2',\n",
              " 'factor_3',\n",
              " 'factor_4',\n",
              " 'factor_5',\n",
              " 'factor_6',\n",
              " 'factor_7',\n",
              " 'factor_8',\n",
              " 'factor_9',\n",
              " 'factor_10',\n",
              " 'target']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "''' Preparing numerical features for scaling '''\n",
        "\n",
        "numerical_features = ['market_cap']+list(data.loc[:,'factor_1':'factor_10'].columns)+['target']\n",
        "numerical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0UF7Xdw1HohN"
      },
      "outputs": [],
      "source": [
        "def scale_and_drop_columns(train_dict, valid_dict, test_dict, numerical_features, drop_extra_factors = g.drop_extra_factors):\n",
        "    ''' Scales the features and target based on the mean/std of the entire training set and possibly drops features\n",
        "    \n",
        "    args:\n",
        "        train_dict, valid_dict, test_dict are dicitonaries\n",
        "        numerical features -- the features/target we wish to scale\n",
        "        drop_extra_factors --- if True, drops all \"factors\" except 2 and 9\n",
        "        \n",
        "    returns:\n",
        "        2 or 3 dictionaries which are scaled and ready for training after an appropriate concatenation\n",
        "        mean and std of input training set features/target'''\n",
        "    \n",
        "    train_df = pd.concat(train_dict, ignore_index = True)\n",
        "    t_mean = np.mean(train_df[numerical_features], axis = 0)\n",
        "    t_std = np.std(train_df[numerical_features], axis = 0)\n",
        "    # each for loop scaled and drops features for each dicitonary entry in train/test/valid\n",
        "    \n",
        "    for i in range(0,len(train_dict)):\n",
        "        train_dict[i][numerical_features] = (train_dict[i][numerical_features]-t_mean)/t_std\n",
        "        train_dict[i].drop(columns = ['identifier'], inplace = True)\n",
        "        if drop_extra_factors ==True:\n",
        "          train_dict[i].drop(columns = ['factor_1', 'factor_3', 'factor_4', 'factor_5', 'factor_6', \n",
        "                                        'factor_7', 'factor_8', 'factor_10'], inplace= True)\n",
        "    for i in range(0,len(valid_dict)):\n",
        "        valid_dict[i][numerical_features] = (valid_dict[i][numerical_features]-t_mean)/t_std\n",
        "        valid_dict[i].drop(columns = ['identifier'], inplace = True)\n",
        "        if drop_extra_factors ==True:\n",
        "          valid_dict[i].drop(columns = ['factor_1', 'factor_3', 'factor_4', 'factor_5', 'factor_6', \n",
        "                                        'factor_7', 'factor_8', 'factor_10'], inplace= True)\n",
        "    for i in range(0,len(test_dict)):\n",
        "        test_dict[i][numerical_features] = (test_dict[i][numerical_features]-t_mean)/t_std\n",
        "        test_dict[i].drop(columns = ['identifier'], inplace = True)\n",
        "        if drop_extra_factors ==True:\n",
        "          test_dict[i].drop(columns = ['factor_1', 'factor_3', 'factor_4', 'factor_5', 'factor_6', \n",
        "                                        'factor_7', 'factor_8', 'factor_10'], inplace= True)\n",
        "    return train_dict, valid_dict, test_dict, t_mean, t_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rrrk5hkyMa8G"
      },
      "outputs": [],
      "source": [
        "train_final, valid_final, test_final, t_mean, t_std= \\\n",
        "scale_and_drop_columns(train_dict, valid_dict, test_dict, numerical_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M2EB-noo038",
        "outputId": "2d95db00-0a4f-4960-f6e1-bc34c3759b86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(market_cap    1.111389e+10\n",
              " factor_1      4.985447e-01\n",
              " factor_2     -1.913018e-04\n",
              " factor_3      5.000637e-01\n",
              " factor_4      4.988483e-01\n",
              " factor_5      5.005490e-01\n",
              " factor_6      5.000944e-01\n",
              " factor_7      4.979793e-01\n",
              " factor_8      4.986385e-01\n",
              " factor_9     -2.047397e-04\n",
              " factor_10     4.986891e-01\n",
              " target       -2.058015e-04\n",
              " dtype: float64, market_cap    1.589832e+10\n",
              " factor_1      2.890597e-01\n",
              " factor_2      5.703728e-03\n",
              " factor_3      2.890388e-01\n",
              " factor_4      2.889261e-01\n",
              " factor_5      2.891119e-01\n",
              " factor_6      2.878230e-01\n",
              " factor_7      2.877822e-01\n",
              " factor_8      2.890916e-01\n",
              " factor_9      5.954812e-03\n",
              " factor_10     2.872727e-01\n",
              " target        5.718353e-03\n",
              " dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "t_mean, t_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjvKFFz2H4c3",
        "outputId": "a8c3afd1-2e13-4e34-c248-b11a6d5fb669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 47 entries, 2010-11-10 to 2011-09-28\n",
            "Data columns (total 14 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   market_cap  47 non-null     float64\n",
            " 1   factor_2    47 non-null     float64\n",
            " 2   factor_9    47 non-null     float64\n",
            " 3   sector_10   47 non-null     uint8  \n",
            " 4   sector_15   47 non-null     uint8  \n",
            " 5   sector_20   47 non-null     uint8  \n",
            " 6   sector_25   47 non-null     uint8  \n",
            " 7   sector_30   47 non-null     uint8  \n",
            " 8   sector_35   47 non-null     uint8  \n",
            " 9   sector_40   47 non-null     uint8  \n",
            " 10  sector_45   47 non-null     uint8  \n",
            " 11  sector_50   47 non-null     uint8  \n",
            " 12  sector_55   47 non-null     uint8  \n",
            " 13  target      47 non-null     float64\n",
            "dtypes: float64(4), uint8(10)\n",
            "memory usage: 2.3 KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 0 entries\n",
            "Data columns (total 15 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   date        0 non-null      object\n",
            " 1   market_cap  0 non-null      object\n",
            " 2   factor_2    0 non-null      object\n",
            " 3   factor_9    0 non-null      object\n",
            " 4   sector_10   0 non-null      object\n",
            " 5   sector_15   0 non-null      object\n",
            " 6   sector_20   0 non-null      object\n",
            " 7   sector_25   0 non-null      object\n",
            " 8   sector_30   0 non-null      object\n",
            " 9   sector_35   0 non-null      object\n",
            " 10  sector_40   0 non-null      object\n",
            " 11  sector_45   0 non-null      object\n",
            " 12  sector_50   0 non-null      object\n",
            " 13  sector_55   0 non-null      object\n",
            " 14  target      0 non-null      object\n",
            "dtypes: object(15)\n",
            "memory usage: 0.0+ bytes\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 13 entries, 2011-10-05 to 2011-12-28\n",
            "Data columns (total 14 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   market_cap  13 non-null     float64\n",
            " 1   factor_2    13 non-null     float64\n",
            " 2   factor_9    13 non-null     float64\n",
            " 3   sector_10   13 non-null     uint8  \n",
            " 4   sector_15   13 non-null     uint8  \n",
            " 5   sector_20   13 non-null     uint8  \n",
            " 6   sector_25   13 non-null     uint8  \n",
            " 7   sector_30   13 non-null     uint8  \n",
            " 8   sector_35   13 non-null     uint8  \n",
            " 9   sector_40   13 non-null     uint8  \n",
            " 10  sector_45   13 non-null     uint8  \n",
            " 11  sector_50   13 non-null     uint8  \n",
            " 12  sector_55   13 non-null     uint8  \n",
            " 13  target      13 non-null     float64\n",
            "dtypes: float64(4), uint8(10)\n",
            "memory usage: 650.0 bytes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "'''checking organization for various dictionary entries '''\n",
        "\n",
        "train_final[300].info(), valid_final[0].info(), test_final[300].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pf3Iuy21Z6V9"
      },
      "outputs": [],
      "source": [
        "def shuffle_and_aggregate(data_dict, shuffle_ = True):\n",
        "    ''' shuffles if shuffle = True and concatenates all entries of the input dictionary '''\n",
        "\n",
        "\n",
        "    data_agg = pd.concat(data_dict, axis = 0)\n",
        "    if shuffle_ == True:\n",
        "        data_agg = shuffle(data_agg)\n",
        "    target = data_agg['target']\n",
        "    data_agg = data_agg.drop(columns = 'target')\n",
        "    return data_agg, target\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "svYn4F7OopAN"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = shuffle_and_aggregate(train_final)\n",
        "x_valid, y_valid = shuffle_and_aggregate(valid_final, shuffle_= False)\n",
        "x_test, y_test = shuffle_and_aggregate(test_final, shuffle_=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "20A-rW1a4iZc"
      },
      "source": [
        "## Model testing and evaluation\n",
        "* We train and test the data on a barrage of models, including:\n",
        "    * baseline models such as predict last value, predict last year's value, or predict 0\n",
        "    * a linear model\n",
        "    * a random forest regressor\n",
        "    * gradient boosted trees\n",
        "    * a dense neural network model\n",
        "    * a time dependent LSTM/dense hybrid network\n",
        "* The main metrics we use are:\n",
        "    * SMAPE = $$\\frac{1}{m}\\sum_1^m\\frac{2\\vert y_{true}-y_{pred}\\vert}{\\max(\\vert y_{true}\\vert+\\vert y_{pred}\\vert +\\epsilon,\\, threshold+\\epsilon)})$$\n",
        "    \n",
        "    * MAPE = $$\\frac{1}{m}\\sum_1^m\\frac{\\vert y_{true}-y_{pred}\\vert}{\\max(\\vert y_{true}\\vert +\\epsilon,\\, threshold+\\epsilon)})$$\n",
        "        both with a threshold of 0.5, and\n",
        "    \n",
        "    * mean absolute error (mae)\n",
        "    \n",
        "* In retrospect, the trainable models did not perform substantially different in these regards, with the except of the hybrid LSTM model, which performed worse\n",
        "* Recommendation depends on the application, but I personally would choose the random forest model or dense nn, with the latter allowing for further parameter\n",
        "    tuning and customizability after gathering more data in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Q0yBwpIqDKWh"
      },
      "outputs": [],
      "source": [
        "def create_mape_np(epsilon = g.smape_epsilon, threshold = g.smape_threshold, mean = True):\n",
        "    ''' creates a \"mean absolute percentage error\" metric for numeric input\n",
        "        args:\n",
        "        threshold -- value pairs with actual below threshold will be measured like mean absolute error,\n",
        "                    above the threshold scaled as a percentage of actual\n",
        "        epsilon -- to prevent division by zero \n",
        "        mean -- average the errors'''\n",
        "    def mape(y_true, y_pred, **kwargs):\n",
        "        sum_ = np.maximum(np.abs(y_true) + epsilon, threshold+epsilon)\n",
        "        if mean == False:\n",
        "            return 100*np.abs(y_pred - y_true) / sum_\n",
        "        else:\n",
        "            return -np.mean(100*np.abs(y_pred - y_true) / sum_)\n",
        "    return mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "H2BobhObkvRT"
      },
      "outputs": [],
      "source": [
        "def create_smape_np(epsilon = g.smape_epsilon, threshold = g.smape_threshold, mean = True):\n",
        "    ''' creates a \"symmetric mean absolute percentage error\" metric for numeric input\n",
        "        args:\n",
        "        threshold -- value pairs with actual below threshold will be measured like mean absolute error,\n",
        "                    above the threshold scaled as a percentage of actual\n",
        "        epsilon -- to prevent division by zero \n",
        "        mean -- average the errors'''\n",
        "\n",
        "    def smape(y_true, y_pred, **kwargs):\n",
        "        sum_ = np.maximum(np.abs(y_true) + np.abs(y_pred) + epsilon, threshold+epsilon)\n",
        "        if mean == False:\n",
        "            return 100*np.abs(y_pred - y_true) / sum_*2\n",
        "        else:\n",
        "            return -np.mean(100*np.abs(y_pred - y_true) / sum_ *2)\n",
        "    return smape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0r7HXNNEheyU"
      },
      "outputs": [],
      "source": [
        "smape = create_smape_np()\n",
        "mape = create_mape_np()\n",
        "\n",
        "smape_score = make_scorer(create_smape_np(), higher_is_better = False)\n",
        "mape_score = make_scorer(create_mape_np(), higher_is_better = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "n7nrQhLP4iZd"
      },
      "source": [
        "### We test some baseline models to get an idea of what performance we should expect.\n",
        "* Predict zero: This model did the best out of the naive models, which makes sense since there seems to be a lot of\n",
        "    time steps with close to zero return.\n",
        "* Predict last value, predict average of three last values: Performed the worse than predict zero.\n",
        "* Predict last year's value, or the moving average of 3 time steps from last year: Performed the worst, which shows that we do not have yearly seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_s8Hp3EWmt5-"
      },
      "outputs": [],
      "source": [
        "def naive_models(valid_scaled, metrics = {'smape':create_smape_np(mean = False), 'mape': create_mape_np(mean = False)}):\n",
        "    ''' This give a baseline model: using the last value as the future value, the mean of the last three values, and zero\n",
        "    \n",
        "    args:\n",
        "        valid_scaled -- test dictionary\n",
        "        metrics -- metric dictionary '''\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    total_smape_0 = 0\n",
        "    total_smape = 0\n",
        "    total_smape_3 = 0\n",
        "    total_mae_0 = 0\n",
        "    total_mae = 0\n",
        "    total_mae_3 = 0\n",
        "    total_mape_0 = 0\n",
        "    total_mape = 0\n",
        "    count_0=0\n",
        "    count = 0\n",
        "    count_3 = 0\n",
        "    for i in range(0, len(valid_scaled)):\n",
        "        if valid_scaled[i].shape[0]>0:\n",
        "            total_smape_0 += np.sum(metrics['smape'](np.zeros(valid_scaled[i].shape[0], dtype = np.float32), np.asarray(valid_scaled[i].iloc[:,-1])))\n",
        "            total_mape_0 += np.sum(metrics['mape'](np.asarray(valid_scaled[i].iloc[:,-1]),np.zeros(valid_scaled[i].shape[0], dtype = np.float32)))\n",
        "            total_smape += np.sum(metrics['smape'](np.asarray(valid_scaled[i].iloc[1:,-1]), np.asarray(valid_scaled[i].iloc[:-1,-1])))\n",
        "            total_mape += np.sum(metrics['mape'](np.asarray(valid_scaled[i].iloc[1:,-1]), np.asarray(valid_scaled[i].iloc[:-1,-1])))\n",
        "            total_smape_3 += np.sum(metrics['smape'](np.asarray(valid_scaled[i].iloc[3:,-1]),\n",
        "                                                     (1/3)*(np.asarray(valid_scaled[i].iloc[:-3,-1])\n",
        "                                                     +np.asarray(valid_scaled[i].iloc[1:-2,-1])\n",
        "                                                     +np.asarray(valid_scaled[i].iloc[2:-1,-1]))))\n",
        "            total_mae_0 += np.sum(abs(np.zeros(valid_scaled[i].shape[0], dtype = np.float32)-np.asarray(valid_scaled[i].iloc[:,-1])))\n",
        "            total_mae += np.sum(abs(np.asarray(valid_scaled[i].iloc[1:,-1])-np.asarray(valid_scaled[i].iloc[:-1,-1])))\n",
        "            total_mae_3 += np.sum(abs(np.asarray(valid_scaled[i].iloc[3:,-1])-(1/3)*(np.asarray(valid_scaled[i].iloc[:-3,-1])\n",
        "                                                                                  +np.asarray(valid_scaled[i].iloc[1:-2,-1])\n",
        "                                                                                  +np.asarray(valid_scaled[i].iloc[2:-1,-1]))))\n",
        "            count_0+=valid_scaled[i]['target'].shape[0]\n",
        "            count += valid_scaled[i]['target'].shape[0]-1\n",
        "            count_3 += valid_scaled[i]['target'].shape[0]-3\n",
        "        else:\n",
        "            continue\n",
        "    print(f\"Predicting with the mean (zero) gives a SMAPE of {total_smape_0/count_0} \\\n",
        ", an MAE of {total_mae_0/count_0}, and a MAPE of {total_mape_0/count_0}\")\n",
        "    print(f\"Predicting with last value gives a SMAPE of {total_smape/count} \\\n",
        ", an MAE of {total_mae/count}, and a MAPE of {total_mape/count}\")\n",
        "  \n",
        "    print(f\"Predicting with avererage of the last 3 values gives a SMAPE of {total_smape_3/count_3} \\\n",
        "and an MAE of {total_mae_3/count_3}\")\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CHDs6Rrg4iZe",
        "outputId": "c7acf97d-a325-4ae6-f435-ff0dc45dc511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of series to be trained: 271\n",
            "Number of series to be validated: 0\n",
            "Number of series to be tested: 0\n",
            "Number of time steps to be trained: 28184\n",
            "Number of time steps to be validated: 0\n",
            "Number of time steps to be tested: 0\n",
            "Train_proportion: 1.0\n",
            "Predicting with the mean (zero) gives a SMAPE of 67.09561349870656 , an MAE of 0.3615971525588669, and a MAPE of 33.54780674935328\n",
            "Predicting with last value gives a SMAPE of 83.69587629649352 , an MAE of 0.5565116092751653, and a MAPE of 74.72881966818849\n",
            "Predicting with avererage of the last 3 values gives a SMAPE of 93.09097649746099 and an MAE of 0.523842334067982\n"
          ]
        }
      ],
      "source": [
        "''' we run the naive models on the whole data set with only complete series and also on the test set '''\n",
        "\n",
        "naive_test_data, blank, blank_2= split_to_series(data,valid_cut = None, valid = False, test_cut= '2012-01-01', drop_non_complete = True)\n",
        "naive_test_data, blank, blank_2, mean, std = scale_and_drop_columns(naive_test_data, blank, blank_2, numerical_features)\n",
        "\n",
        "naive_models(naive_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gb6ZKSPz4iZf",
        "outputId": "b00d7cfd-929a-4656-b898-957f99af45a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting with the mean (zero) gives a SMAPE of 74.29249034332256 , an MAE of 0.44632912126987584, and a MAPE of 37.14624517166128\n",
            "Predicting with last value gives a SMAPE of 93.40561160014964 , an MAE of 0.7107571052869068, and a MAPE of 92.7638038333415\n",
            "Predicting with avererage of the last 3 values gives a SMAPE of 99.85240650648245 and an MAE of 0.6525911281776178\n"
          ]
        }
      ],
      "source": [
        "''' testing the naive models on the test set, which may have missing time steps '''\n",
        "\n",
        "naive_models(test_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3c0o_xjD4iZf"
      },
      "outputs": [],
      "source": [
        "def seasonal_model(valid_scaled, metrics = {'smape':create_smape_np(mean = False), 'mape': create_mape_np(mean = False)}):\n",
        "    ''' Another baseline model: using last year's value as the future value, \n",
        "        or the centered average window of step size 3 from last year.\n",
        "    \n",
        "    args:\n",
        "        valid_scaled = data_dictionary (ordered by date)\n",
        "        metrics -- metric dictionary '''\n",
        "    total_smape_3=0\n",
        "    total_mae_3=0\n",
        "    total_mape_3=0\n",
        "    count_3=0\n",
        "    \n",
        "    total_smape = 0\n",
        "    total_mae = 0\n",
        "    total_mape = 0\n",
        "    count = 0\n",
        "    for i in range(0, len(valid_scaled)):\n",
        "        if valid_scaled[i].shape[0]>0:\n",
        "            total_smape += np.sum(metrics['smape'](np.asarray(valid_scaled[i].iloc[52:,-1]), np.asarray(valid_scaled[i].iloc[:-52,-1])))\n",
        "            total_mape += np.sum(metrics['mape'](np.asarray(valid_scaled[i].iloc[52:,-1]), np.asarray(valid_scaled[i].iloc[:-52,-1])))\n",
        "            total_smape_3 += np.sum(metrics['smape'](np.asarray(valid_scaled[i].iloc[53:,-1]),\n",
        "                                                     (1/3)*(np.asarray(valid_scaled[i].iloc[1:-52,-1])\n",
        "                                                     +np.asarray(valid_scaled[i].iloc[:-53,-1])\n",
        "                                                     +np.asarray(valid_scaled[i].iloc[2:-51,-1]))))\n",
        "            total_mape_3 += np.sum(metrics['mape'](np.asarray(valid_scaled[i].iloc[53:,-1]),\n",
        "                                                     (1/3)*(np.asarray(valid_scaled[i].iloc[1:-52,-1])\n",
        "                                                     +np.asarray(valid_scaled[i].iloc[:-53,-1])\n",
        "                                                     +np.asarray(valid_scaled[i].iloc[2:-51,-1]))))\n",
        "            total_mae += np.sum(abs(np.asarray(valid_scaled[i].iloc[52:,-1])-np.asarray(valid_scaled[i].iloc[:-52,-1])))\n",
        "            total_mae_3 += np.sum(abs(np.asarray(valid_scaled[i].iloc[53:,-1])-(1/3)*(np.asarray(valid_scaled[i].iloc[1:-52,-1])\n",
        "                                                                                  +np.asarray(valid_scaled[i].iloc[:-53,-1])\n",
        "                                                                                  +np.asarray(valid_scaled[i].iloc[2:-51,-1]))))\n",
        "\n",
        "            count += valid_scaled[i]['target'].shape[0]-52\n",
        "            count_3 += valid_scaled[i]['target'].shape[0]-53\n",
        "        else:\n",
        "            continue\n",
        "    print(f\"Predicting with the the centered window average of 3 steps from last year gives a SMAPE of {total_smape_3/count_3} \\\n",
        ", an MAE of {total_mae_3/count_3}, and a MAPE of {total_mape_3/count_3}\")\n",
        "    print(f\"Predicting with last year's value gives a SMAPE of {total_smape/count} \\\n",
        ", an MAE of {total_mae/count}, and a MAPE of {total_mape/count}\")\n",
        "\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RwoFegqL4iZf",
        "outputId": "21c10093-f93b-4052-abec-920071a47358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting with the the centered window average of 3 steps from last year gives a SMAPE of 104.28726000463186 , an MAE of 0.599460701330063, and a MAPE of 72.87570950538543\n",
            "Predicting with last year's value gives a SMAPE of 95.97756634999791 , an MAE of 0.6345810408792786, and a MAPE of 79.30535626127818\n"
          ]
        }
      ],
      "source": [
        "''' Testing the seasonal model on the set of complete time series \n",
        "    very poor performance in retrospect, indicating no yearly seasonality'''\n",
        "\n",
        "seasonal_model(naive_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "dp15fhoP4iZg"
      },
      "source": [
        "### Classical models\n",
        "* We train and test the data on classical models: linear regression, random forests, and gradient boosted trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhjrRTrf7sQh",
        "outputId": "8adbb40f-f4de-4a75-cb8c-f55d049b5a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mae: 0.2466591729495038  smape: -32.38065740919386  mape: -24.955454306226073\n"
          ]
        }
      ],
      "source": [
        "'''trying first real model \n",
        "    degree optimized on earlier validation set'''\n",
        "\n",
        "poly = PolynomialFeatures(degree = 1)\n",
        "poly_train = poly.fit_transform(x_train)\n",
        "poly_test = poly.transform(x_test)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(poly_train, y_train)\n",
        ", , \n",
        "print(\"mae:\", mean_absolute_error(lin_reg.predict(poly_test), y_test), \" smape:\", smape(lin_reg.predict(poly_test), y_test), \" mape:\", mape(y_test, lin_reg.predict(poly_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9km13t2_fMuJ"
      },
      "outputs": [],
      "source": [
        "def model_cv(model, parameters, x_train, y_train, x_valid, y_valid, scoring = smape_score, refit = 'smape', verbose = False):\n",
        "    ''' small function which runs grid search cross validation according to preference '''\n",
        "    \n",
        "    model = GridSearchCV(model, param_grid=parameters, scoring = scoring, refit = refit, cv = 3)\n",
        "    model.fit(x_train, y_train)\n",
        "    performance = model.score(x_valid, y_valid)\n",
        "    if verbose ==False:\n",
        "        print(f'{model.best_params_}\\n cv_score = {model.best_score_}\\n valid_score = {performance}'),\n",
        "        return model\n",
        "    \n",
        "    else:\n",
        "        print(f'{model.best_params_}\\n cv_score = {model.best_score_}\\n valid_score = {performance}\\n'), pd.DataFrame(model.cv_results_).sort_values('mean_test_score'), \n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "teNb12UdeDvV",
        "outputId": "598af055-fad4-42c7-ac86-a6b8193517be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' cross validation on the train set was used to optimize parameters for the random forest model '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "''' cross validation on the train set was used to optimize parameters for the random forest model '''\n",
        "\n",
        "# randforest = RandomForestRegressor(n_jobs = -1, random_state = 7)\n",
        "# parameters = {#'n_estimators':[50, 100, 200], \n",
        "#               'max_depth': [6,8,10],\n",
        "#      #'min_samples_split': [2,4,6],\n",
        "#      'min_samples_leaf': [8,10,15 ],\n",
        "#      #'criterion' :['absolute_error', 'squared_error']\n",
        "#      }\n",
        "# model_cv(randforest, parameters, x_train, y_train, x_valid, y_valid, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VzbHkJ3qNz3",
        "outputId": "88426cf8-c7a0-4943-d6e1-e23e99667cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mae: 0.24936700685262403  smape: -31.558674564608882  mape: -26.719965560993256\n"
          ]
        }
      ],
      "source": [
        "''' refitting random forest with best parameters on the entire train set and evaluating on test'''\n",
        "\n",
        "randforest_test = RandomForestRegressor(n_jobs = -1, random_state = 8, n_estimators = 200, max_depth = 8, min_samples_leaf = 15)\n",
        "randforest_test.fit(x_train, y_train)\n",
        "y_pred = randforest_test.predict(x_test)\n",
        "print(\"mae:\", mean_absolute_error(y_pred, y_test), \" smape:\", smape(y_test, y_pred), \" mape:\", mape(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c-LRP14xdeX",
        "outputId": "c68b57d3-c4c2-4c20-de07-c7cdfd392fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:29:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:29:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:30:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:31:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:32:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:33:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:34:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:35:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:36:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:37:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:38:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:39:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:39:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:39:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:39:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:39:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[07:39:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "{'grow_policy': 'depthwise', 'max_depth': 6, 'min_child_weight': 25, 'subsample': 0.5, 'tree_method': 'hist'}\n",
            " cv_score = -0.20617143586794384\n",
            " valid_score = -0.25555585770270495\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from xgboost.sklearn import XGBRegressor\n",
        "''' fitting with gradient-boosted random forest (has l2 regularization built in)'''\n",
        "param = {'max_depth': [6,8,10], 'min_child_weight': [15,20, 25], \n",
        "         'tree_method':['approx', 'hist'], 'grow_policy':['depthwise', 'lossguide'],\n",
        "         'subsample':[0.5, 1]}\n",
        "xgb_reg = XGBRegressor(num_boost_round = 100, early_stopping_rounds = 10,\n",
        "            metric = 'mae', learning_rate = .1, num_jobs = -1)\n",
        "xgb_reg = model_cv(xgb_reg, param, x_train, y_train, x_test, y_test, verbose = True, scoring = 'neg_mean_absolute_error', refit= True)\n",
        "\n",
        "#dtrain = xgboost.DMatrix(np.asarray(x_train), label= np.asarray(y_train))\n",
        "\n",
        "#xgboost.cv(param,dtrain,nfold = 3,  metrics = 'mae', num_boost_round = 50, early_stopping_rounds = 10)\n",
        "#xgb_reg.fit(X = np.asarray(x_train), y = np.asarray(y_train), eval_set = [(np.asarray(x_test), np.asarray(y_test))],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Fu00YKFd1vSJ"
      },
      "outputs": [],
      "source": [
        "smape = create_smape_np(mean = True)\n",
        "mape = create_mape_np(mean = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1DeqaYd22DM",
        "outputId": "f651355f-c82b-4bc8-869b-70963c6abdbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mae: 0.25555585770270495  smape: -31.81289652123448  mape: -26.961398520684597\n"
          ]
        }
      ],
      "source": [
        "''' performance of gradient boosted trees '''\n",
        "\n",
        "\n",
        "y_pred =xgb_reg.predict(x_test)\n",
        "print(\"mae:\", mean_absolute_error(np.asarray(y_test), y_pred), \" smape:\", smape(np.asarray(y_test), y_pred), \" mape:\", mape(np.asarray(y_test), y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "UY0chRek4iZi"
      },
      "source": [
        "### Neural network models\n",
        "* we train and test thje data on two neural networks models: a dense model and a hybrid LSTM/Dense model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfPPkhBlPJtN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "''' smape metric for output of neural networks '''\n",
        "\n",
        "def create_smape(epsilon = g.smape_epsilon, threshold = g.smape_threshold):\n",
        "    def smape(y_true, y_pred):\n",
        "        summ = K.maximum(K.abs(y_true) + K.abs(y_pred)+epsilon, threshold + epsilon)\n",
        "        return 100*K.abs(y_pred - y_true) / summ *2\n",
        "    return smape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9hxxOOIF2gD"
      },
      "outputs": [],
      "source": [
        "''' mape metric for output of neural networks '''\n",
        "\n",
        "def create_mape(epsilon = g.smape_epsilon, threshold = g.smape_threshold):\n",
        "    def mape(y_true, y_pred):\n",
        "        summ = K.maximum(K.abs(y_true)+epsilon, threshold + epsilon)\n",
        "        return 100*K.abs(y_pred - y_true) / summ\n",
        "    return mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX1qJtANnd4V"
      },
      "outputs": [],
      "source": [
        "def plot_model_perf(history, metric = None):\n",
        "    ''' plots the loss and metric curves of a neural network model with regard to epoch '''\n",
        "    \n",
        "    plt.subplots(len([metric]), 1, figsize=(12, 8))\n",
        "    epochs = range(len(history.history['loss']))\n",
        "    plt.plot( epochs, history.history['loss'],color = 'blue', label = 'loss' )\n",
        "    plt.plot(epochs, history.history['val_loss'], color = 'red', label = 'val_loss' )\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    if metric:\n",
        "        for i, metric in enumerate(metric):\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.plot( epochs, history.history[f'{metric}'],color = 'green', label = f'{metric}' )\n",
        "            plt.plot(epochs, history.history[f'val_{metric}'], color = 'orange', label = f'val_{metric}' )\n",
        "            plt.legend()\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tOV0TBchQLK"
      },
      "outputs": [],
      "source": [
        "def dense_block(input_, dense_units, dr):\n",
        "    ''' A dense 2 layer block with relu activation, dropout, and regularization.\n",
        "        args:\n",
        "        --dense_units -- number of neurons per layer\n",
        "        --dr -- dropout rate '''\n",
        "    lrelu= tf.keras.layers.LeakyReLU()\n",
        "    x = Dense(dense_units, kernel_initializer=\"he_normal\", kernel_regularizer='l2'\n",
        "              )(input_)\n",
        "    x = Dropout(dr)(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(dense_units, kernel_initializer=\"he_normal\", kernel_regularizer='l2'\n",
        "              )(x)\n",
        "    x = Dropout(dr)(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    out = Activation('relu')(x)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zlFlLH_gLn2"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def create_model(dense_units = 64, dr = .2 ):\n",
        "    ''' creating a model from the dense block \n",
        "        and a single output in order to do regression '''\n",
        "    input_ = Input(shape=(x_train.shape[1]))\n",
        "    x = dense_block(input_, dense_units, dr)\n",
        "    out = Dense(1)(x)\n",
        "    model = Model(inputs = input_, outputs = out)\n",
        "    return model\n",
        "\n",
        "'''experimenting with different optimizers. Performance difference was negligible '''\n",
        "adam = tf.keras.optimizers.Adam()\n",
        "model = create_model()\n",
        "nadam = tf.keras.optimizers.Nadam(learning_rate=.001)\n",
        "rmsprop = tf.keras.optimizers.RMSprop(learning_rate = .001, momentum=.9)\n",
        "model.compile(loss = 'mae', optimizer = 'adam', metrics = ['mae', create_mape(),create_smape()])\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr6O7atenJke"
      },
      "outputs": [],
      "source": [
        "''' training cell: early stopping implemented, lr_tuning was used to tune the learning rate of different optimizers '''\n",
        "\n",
        "early_stopping = EarlyStopping(patience = 7, restore_best_weights= True)\n",
        "epochs = 100\n",
        "lr_tuning = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**((4*epoch)/epochs))\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = epochs, validation_data = [x_test, y_test],\n",
        "                    callbacks = [early_stopping], batch_size = g.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKZosgRxMGt3"
      },
      "outputs": [],
      "source": [
        "''' used to evaluate learning rate changes '''\n",
        "\n",
        "\n",
        "# lrs = 1e-5 * (10 ** (4*np.arange(epochs) / epochs))\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.grid(True)\n",
        "# plt.semilogx(lrs, history.history[\"loss\"])\n",
        "# plt.tick_params('both', length=10, width=1, which='both')\n",
        "# plt.axis([1e-5, 1e-5 * 10**(4), 0, epochs])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeBY85zw17p3"
      },
      "outputs": [],
      "source": [
        "plot_model_perf(history, ['mae','mape','smape'])\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uco9LAaLMslv"
      },
      "outputs": [],
      "source": [
        "''' we analyze the weights on the input to the first layer of the neural network.\n",
        "    the first three are \"market_cap\", \"factor_2\", and \"factor_9\"\n",
        "    When the model was run with all of the factors, the weights on the other factors were very small '''\n",
        "\n",
        "\n",
        "x = np.squeeze(model.layers[1].get_weights()[0])\n",
        "print(x.shape)\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.xticks(range(0,x.shape[0]))\n",
        "#plt.yscale('log')\n",
        "plt.plot(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "DLoQ8iMQ4iZl"
      },
      "source": [
        "#### Traing and testing a time model\n",
        "I decided to try to use the information from the previous g.window_size time steps to train a\n",
        "hybrid LSTM/Dense model. The dataset was separated differently: Notably, only firms with\n",
        "104 time steps were considered for ease of organization. The model is compared with the Dense model\n",
        "on the same data set.\n",
        "\n",
        "In previous revisions, the entire feature set for the previous g.window_size\n",
        "time steps was fed to recurrent network, but this only ended up predicting the constant time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XykP6VVRN1rv"
      },
      "outputs": [],
      "source": [
        "''' finally, we build a model which takes in the features for a given time step, as well as the targets from \n",
        "    the previous g.window_size time steps'''\n",
        "\n",
        "train_dict_time, valid_dict_time, test_dict_time = split_to_series(data, test_cut = '2011-06-01', drop_non_complete = True)\n",
        "train_final_time, valid_final_time, test_final_time, t_mean, t_std= \\\n",
        "scale_and_drop_columns(train_dict_time, valid_dict_time, test_dict_time, numerical_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTVV5Z7KuTyU"
      },
      "outputs": [],
      "source": [
        "def split_data_for_time_model(data_dict, window_size = g.window_size):\n",
        "    ''' for each entry in data_dict, adds features which are the previous\n",
        "        window_size targets \n",
        "        separates into two feature sets: data_static with features for target time step \n",
        "        and data_window with previous time steps\n",
        "        also returns targets'''\n",
        "    \n",
        "    \n",
        "    static_data = data_dict[0].iloc[window_size:,:-1]\n",
        "    targets = data_dict[0]['target'].iloc[window_size:]\n",
        "    windows = data_dict[0][['target']].iloc[0:window_size].reset_index(drop = True).transpose()\n",
        "    for i in range(1, len(data_dict[0])-window_size):\n",
        "        window = data_dict[0][['target']].iloc[i:window_size+i].reset_index(drop = True).transpose()\n",
        "        windows = pd.concat((windows, window), axis = 0)\n",
        "    def make_windows(data, windows, targets, window_size = window_size):\n",
        "        target = data['target'].iloc[window_size:]\n",
        "        for i in range(0, len(data)-window_size):\n",
        "            window = data[['target']].iloc[i:window_size+i].reset_index(drop = True).transpose()\n",
        "            windows = pd.concat((windows, window), axis = 0)\n",
        "        targets = pd.concat([targets, target], axis = 0)\n",
        "        return windows, targets\n",
        "    for i in range(1, len(data_dict)):\n",
        "        static_temp = data_dict[i].iloc[window_size:,:-1]\n",
        "        static_data = pd.concat([static_data, static_temp], axis = 0)\n",
        "        windows, targets = make_windows(data_dict[i], windows, targets)\n",
        "    return np.asarray(static_data.reset_index(drop= True)), np.asarray(windows.reset_index(drop=True)), np.asarray(targets.reset_index(drop= True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbtX0tke3p6w",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_static, train_windows, train_targets= split_data_for_time_model(train_final_time)\n",
        "test_static, test_windows, test_targets = split_data_for_time_model(test_final_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzWfc1Us4LYr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_static.shape, train_windows.shape, train_targets.shape, test_static.shape, test_windows.shape, test_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QEEhnmJ4iZl"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(train_windows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs98Vb9j3wKF"
      },
      "outputs": [],
      "source": [
        "np.count_nonzero(np.isnan(train_targets)), np.count_nonzero(np.isnan(train_windows)), np.count_nonzero(np.isnan(test_targets)), np.count_nonzero(np.isnan(test_windows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYDfC-vQV20M"
      },
      "outputs": [],
      "source": [
        "# def convert_to_tensor(data_dict, window_size = g.window_size, batch_size = g.batch_size, shuffle_buffer = 1000, shuffle = True):\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices(np.asarray(data_dict[0], dtype= np.float32))\n",
        "#     dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "#     dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "#     dataset = dataset.map(lambda window: (window[:-1], window[-1,-1]))\n",
        "#     for i in range(1, len(data_dict)):\n",
        "#       dataset_tmp = tf.data.Dataset.from_tensor_slices(np.asarray(data_dict[i], dtype= np.float32))\n",
        "#       dataset_tmp = dataset_tmp.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "#       dataset_tmp = dataset_tmp.flat_map(lambda window: window.batch(window_size + 1))\n",
        "#       dataset_tmp = dataset_tmp.map(lambda window: (window[:-1], window[-1,-1]))\n",
        "#       dataset = dataset.concatenate(dataset_tmp)\n",
        "#     if shuffle == True:\n",
        "#       dataset = dataset.shuffle(shuffle_buffer)\n",
        "#     dataset = dataset.batch(batch_size).prefetch(1)\n",
        "#     return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNzfQ_99aDCv"
      },
      "outputs": [],
      "source": [
        "# train_time, train_static = convert_to_tensor(train_scaled)\n",
        "# valid_time, valid_static = convert_to_tensor(valid_scaled, shuffle = False)\n",
        "# test_time, test_static = convert_to_tensor(test_scaled, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8xTTTddggmd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def conv_block(input_,filter_units,kernel_size,dr):\n",
        "    '''basic conv1D block, used for experimenting '''\n",
        "    x = Conv1D(filters = filter_units, kernel_size = kernel_size, strides = 1,\n",
        "                                                              kernel_initializer='he_normal', \n",
        "               kernel_regularizer = 'l2', padding = 'causal',activation = 'relu')(input_)\n",
        "    # x = Dropout(dr)(x)\n",
        "    # x = MaxPooling1D()(x)\n",
        "    # x = Conv1D(filters = filter_units, kernel_size = kernel_size, strides = 1,\n",
        "    #                                                           kernel_initializer='he_normal', padding = 'causal',activation = 'relu')(x)\n",
        "    out = Dropout(dr)(x)\n",
        "    #out = MaxPooling1D()(x)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dMy-YD8g3kB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def lstm_block(input_,lstm_units, dr):\n",
        "    ''' lstm layer with dropout and lstm_units neurons '''\n",
        "    #x = LSTM(lstm_units, return_sequences = True, dropout = dr)(input_)\n",
        "    out = LSTM(lstm_units, return_sequences = False, dropout = dr, recurrent_dropout = dr, \n",
        "               kernel_regularizer='l2', recurrent_regularizer='l2')(input_)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7kw2JFTL1cX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "\n",
        "def create_time_model(filter_units = 64, lstm_units = 8, kernel_size = 4, dense_units = 64, dr = .2 ):\n",
        "    ''' creates a 2 input model which takes in a time window and feeds it to an LSTM layer,\n",
        "        concanenates with the output of a dense block. The dense block takes in the features for\n",
        "        a given time step. Gives a single output for regression '''\n",
        "    \n",
        "    \n",
        "    input0 = Input(shape = (train_static.shape[1],), name = 'static')\n",
        "    input1 = Input(shape = (None, 1), name = 'time')\n",
        "    #x = conv_block(input1,filter_units, kernel_size, dr)                                                            \n",
        "    x = lstm_block(input1, lstm_units, dr)\n",
        "    \n",
        "    \n",
        "    y = dense_block(input0, dense_units, dr)\n",
        "    #x = Flatten()(x)\n",
        "    x = Concatenate()([y, x])\n",
        "    x = Dense(16, activation = 'relu', kernel_initializer=\"he_normal\", kernel_regularizer='l2')(x)\n",
        "    x = Dropout(dr)(x)\n",
        "    out = Dense(1)(x)\n",
        "    model = Model(inputs = [input0,input1], outputs = out)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "nadam = tf.keras.optimizers.Nadam(learning_rate=.001)\n",
        "rmsprop = tf.keras.optimizers.RMSprop(learning_rate = .001, momentum=.9)\n",
        "time_model = create_time_model()\n",
        "time_model.compile(loss = 'mae', optimizer = 'adam', metrics = ['mae',create_mape(), create_smape()])\n",
        "time_model.summary()\n",
        "tf.keras.utils.plot_model(time_model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfGMhnH6L1cX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(patience = 5, restore_best_weights= True)\n",
        "history = time_model.fit(x = {'static':train_static, 'time':train_windows}, y= train_targets, shuffle = True,\n",
        "                    validation_data = [{'static':test_static, 'time':test_windows}, test_targets], \n",
        "                    batch_size = g.batch_size, epochs = 100, callbacks = early_stopping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm07XR1MQ6n4"
      },
      "outputs": [],
      "source": [
        "plot_model_perf(history, ['mae','mape','smape'])\n",
        "time_model.evaluate({'static':test_static, 'time':test_windows}, test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlG7ZbxfRdkk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "''' Weights for the input to the \"static\" dense later. Similar to the Dense nn model. '''\n",
        "\n",
        "x = np.squeeze(time_model.layers[1].get_weights()[0])\n",
        "print(x.shape)\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.xticks(range(0,x.shape[0]))\n",
        "#plt.yscale('log')\n",
        "plt.plot(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4Z7OLYOo4iZn"
      },
      "outputs": [],
      "source": [
        "''' This plot shows the inputs to the dense layer after concatenation. The weights for the LSTM output are at 64 and onward'''\n",
        "\n",
        "x = np.squeeze(time_model.layers[-3].get_weights()[0])\n",
        "print(x.shape)\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.xticks(range(0,x.shape[0]+1, 4))\n",
        "#plt.yscale('log')\n",
        "plt.plot(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9AuvEXY4iZn"
      },
      "outputs": [],
      "source": [
        "''' we compare the time_model to the dense model on the same training and test sets '''\n",
        "\n",
        "x_train_t, y_train_t = shuffle_and_aggregate(train_dict_time)\n",
        "\n",
        "x_test_t, y_test_t = shuffle_and_aggregate(test_dict_time, shuffle_=False)\n",
        "compare_time_model = create_model(dense_units =64)\n",
        "compare_time_model.compile(loss = 'mae', optimizer = 'adam', metrics = ['mae', create_mape(),create_smape()])\n",
        "compare_time_model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(patience = 7, restore_best_weights= True)\n",
        "epochs = 100\n",
        "lr_tuning = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**((4*epoch)/epochs))\n",
        "\n",
        "\n",
        "\n",
        "history = compare_time_model.fit(x_train_t, y_train_t, epochs = epochs, validation_data = [x_test_t, y_test_t],\n",
        "                    callbacks = [early_stopping], batch_size = g.batch_size)\n",
        "\n",
        "plot_model_perf(history, ['mae','mape','smape'])\n",
        "compare_time_model.evaluate(x_test_t, y_test_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "I5rCtL4D4iZo"
      },
      "source": [
        "## Plotting some predictions\n",
        "* We plot some predictions vs. true values for the dense nn, random forest, and linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOT6nXDLcYSk"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(data, dict_number, model = model):\n",
        "    ''' plotting prediction curves. Takes in a data dictionary and a non-negative integer as a key and outputs\n",
        "        a graph of predicted vs. true values over time '''\n",
        "    \n",
        "    plt.figure(figsize = (20, 8))\n",
        "    test_batch = data[dict_number].iloc[:,:-1]\n",
        "    if test_batch.shape[0] == 0:\n",
        "        return print(f'no data point for validation series {dict_number}')\n",
        "    targets = data[dict_number].iloc[:,-1].values\n",
        "    test_pred = model.predict(test_batch)\n",
        "    plt.plot(data[dict_number].index.values, test_pred.reshape(-1), color = 'red', label = 'Prediction')\n",
        "    plt.plot(data[dict_number].index.values, targets.reshape(-1), color = 'green', label = 'Actual')\n",
        "    plt.title(f'Prediction vs. actual for test series {dict_number}')\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpBtHLIRNNkW"
      },
      "outputs": [],
      "source": [
        "''' predictions for the dense model '''\n",
        "for i in range(0, 5):\n",
        "      plot_predictions(test_final, i, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtppo0qQRXMQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "''' predictions for the random forest model '''\n",
        "for i in range(0, 5):\n",
        "    plot_predictions(test_final, i, model = randforest_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MUY9RyXT1Og"
      },
      "outputs": [],
      "source": [
        "'''predictions for the linear model '''\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x_train, y_train)\n",
        "for i in range(0, 5):\n",
        "    plot_predictions(test_final, i, model = lin_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRCJqrZkuVe1"
      },
      "outputs": [],
      "source": [
        "''' predict on random test series '''\n",
        "\n",
        "def plot_random_predictions(n, data, model = model):\n",
        "    ''' plots n random predictions from the given data dicitonary '''\n",
        "    if n <= 0:\n",
        "        return\n",
        "    \n",
        "    r = np.random.randint(0, len(data))\n",
        "    if data[r].shape[0] >0:\n",
        "        plot_predictions(data,r, model = model)\n",
        "        plt.show()\n",
        "        plot_random_predictions(n-1, data, model)\n",
        "    else:\n",
        "        plot_random_predictions(n, data, model)\n",
        "\n",
        "plot_random_predictions(10, test_final)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Preprocessing_and_model_testing_w_gbtrees.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}